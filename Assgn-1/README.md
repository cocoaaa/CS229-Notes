##Notes and thoughts on Assignment-1.
* In Logistic Regreassion, the separation line is same for both, labels {0, 1} and {-1, 1}. Only the regression formula
is tweaked a little bit - Read [here](www.hongliangjie.com/wp-content/uploads/2011/10/logistic.pdf) for in-depth explanation.
* For Pset-1 problem on Logistic regression via Newton's Method, somehow initializing theta with all 1s only seems to work.
* For notes on Maximum Likelihood Estimation for Logistic Regression, an in-depth explanation is - [here](http://sites.stat.psu.edu/~jiali/course/stat597e/notes2/logit.pdf).
* Trick for solving Problem 3 on Gaussian Discrimant Analysis, think in terms of Indicator Random Variables. The labels here are {1, -1}. In this case formula is ![this] (https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/CodeCogsEqn.gif).
* The final likelihood equation for Problem 3 in terms of given paramters (when n=1) ![is] (https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/FinalLikelihoodEquation.png).
* Differentiating the likelihood wrt to the parameters gives the answer, pretty straightforward, no suprised or tricks required.
* The cost function J wrt theta for the quasar regression problem ![is] (https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/Normal_eqn_with_W.png)
* The wrong way to take the trace derivative ![is] (https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/wrong_trace_derivative.png) and
the right way to take the trace derivative ![is] (https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/correct_trace_derivative.png).
* For the sub-problem (b) for the quasar problem, my answer was pretty close to the one given in the key, when you round off its the exact same answer and is bound to occur because of floating point calculations. The code in ```Python``` is ![here] (https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/quasar.py) and the regression line and raw data points are ![here] (https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/figure_1-1.png). 
* The values for ```theta_0 (intercept)``` and ```theta_1 (slope)``` term come out to be ```2.5133990556``` and ```-0.000981122145459``` pretty close to ```2.5134, âˆ’0.0010```, in fact as I said if you round it off its the exact same answer. Calculations were straightforward. The ```X``` or design matrix here would be the very first row with all wavelengths and **don't forget** to add ```X_0=1``` the dummy attribute to calculate the intercept term. The ```Y``` or the response variable would be each row, in the given problem you're told to take the first row after the wavelengths.
* The experiment with locally weighted regression was interesting. The task was to vary the bandwidth parameter from the list 1, 5, 10, 100 and 1000 and observe what happens to the fit. As I increased the band-width parameter, the curve degenerated to a line i.e. unweighted linear regression (check the last plot) and our learning algorithm's behavior changed from ```overfitting``` in the case of ```tau=1``` to ```underfitting`` in the case of ```tau=1000```, in between at the value of 5 and 100 was the curve giving a reasonable fit. The code for locally weighted regression and the  plots is [here](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/lwr.py) The plots for tau are ![tau=1](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/tau%3D1.png) and ![tau=5](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/tau_5.png) and ![tau=10](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/tau%3D10.png) and ![tau=100](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/tau%3D100.png) and ![tau=1000](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-1/tau%3D1000.png).
