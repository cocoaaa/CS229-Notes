##Notes and thoughts on Assignment-2.
* For SVMs read Notes. 
* Solutions for Problem 1 are ![here](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-2/CodeCogsEqn.png).
* Most of them boil down to deriving the form ![here](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-2/PSD.png), you express it as a square of sums, which will always be greater than equal to zero.
* Matlab code for Naive Bayes with Multinomial Event model is ![here](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-2/nb_train.m), pretty straightforward. The results after training on datasets of sizes (no. of documents) 50, 100, 200, 400, 800, 1400, 2144 are, in percentages, 3.87,2.62, 2.62, 1.87, 1.75, 1.63, 1.63. Its the exact same answer as given in the key. The test data is weak, if you know what I mean!.
* To find the 5 most frequent keywords in spam mails as mentioned in the problem, I just calculated the heuristic (the log of the ratio) wrote them to a txt file, read it in python and sorted according the log of ratio. Don't ask, MATLAB is a huge pain. The output file and python file to read the results are ![here](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-2/out.txt) and ![here](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-2/red.py) and the answers are  ```httpaddr, spam, unsubscribe, ebai, valet``` which match exactly with the ones in the key. The Python file will give you indices of the token which you then need to compare with the ones in TOKENS_LIST.
* For 4-a, Answer is True, Assume ```H1``` is set of classifiers with linear decision boundaries and ```H2``` is classifiers with non-linear decision boundaries. Clearly, H1 is a subset of H2. And ```VC(H1) <= VC(H2)```.
* For 4-b, Answer is False. Take ```H2``` as a constant classifier i.e. whose VC dimension is zero and take ```h1``` as a classifier with VC dimension as infinite. Clearly, ```VC(H1) <= VC(H2) + k``` in this case k=1, doesn't hold.
* For 4-c, Answer is True. Assume ```H2``` as classifiers with linear decision boundary and ```H3``` are classifiers with non-linear decision boundaries. ```VC(H1) = max(VC(H2), VC(H3) )``` in this case and ```max(VC(H2), VC(H3) ) <= VC(H2) + VC(H3)```.
* For 5, We start with this - ![this](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-2/Z_tau.png) which is then converted to 
![this](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-2/Z_tau_1.png). We do the conversion because originally we were sampling from ```D_tau``` and we are then converting it to the form where we are sampling from ```D```. The entire derivation is given here - ![here](https://github.com/sudk1896/CS229-Notes/blob/master/Assgn-2/Office%20Lens%2020170115-203541.jpg). For 5-c, As ```Tau``` approaches 0.5, the generalization error also reaches 0.5 and for generalization error to be close to training error with high probability we require an infinite amount of training data (See the derivation for 5-b).

